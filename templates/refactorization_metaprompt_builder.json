{
  "name": "Refactorization Meta-Prompt Builder",
  "content": "Objective\nProduce a single, comprehensive \u201crefactorization meta-prompt\u201d that a modern LLM can execute to analyze and optimize a codebase without assuming a specific runtime environment or executing untrusted code.\n\nContext\nMUST INCLUDE (verbatim intent from SPEC):\nI oftentimes use an LLM to code. I want a model to identify optimizations, security flaws, and ways the code could improve. It should be agnostic of environment, only suggest optimizations, and read the entire codebase and analyze everything in the code for increased speed, security, test coverage, and outdated or inefficient code (i.e. dead code, or code that can be modified and/or optimized). It should identify opportunities for file splits, moving files into relevant directories, and also determining documentation updates relevant to what the code does now.\n\nInputs\n- Focus Areas (optional): {{focus}}\n\nAssumptions\n- Implementer model has full workspace read-only access, no untrusted code execution.\n- If follow-ups go unanswered, proceed with conservative defaults recorded under ASSUMPTIONS and CONSTRAINTS.\n- All proposals must be environment-agnostic: suggest changes, diffs, tests, and commands without local setup assumptions.\n- Refactoring should favor minimal, safe diffs first; larger extractions must include migration/rollback steps.\n- Risk evaluation: perform YOLO-first discovery to surface all possible changes, then default recommendations to SAFE unless user explicitly opts for YOLO or approves risky items.\n\nConstraints\n- Maintain backward compatibility unless explicitly approved.\n- File size targets: \u2264 300\u2013400 LOC per file; \u2264 75 LOC per function unless justified.\n- Reuse existing logic; prefer extraction (function/module) over appending to large files.\n- No secret logging; do not hard-code credentials.\n- Do not upgrade/downgrade dependencies unless explicitly requested.\n- Treat {{focus}} as authoritative when provided.\n\nInstructions\nAct as a prompt engineer orchestrator. Perform three phases, then output one final meta-prompt:\n\n- Phase 0 \u2014 Codebase Ingestion (silent): Read {{focus}}. Do not emit artifacts.\n\n- Phase 1 \u2014 Questioning Loop (15 total):\n  \u2022 Exactly 3 rounds of 5 questions each.\n  \u2022 Questions must progressively refine risk, discovery, and scope.\n  \u2022 Draw on the **Clarity Checklist** (see below).\n  \u2022 If the user says \u201cproceed\u201d without answering, continue with conservative defaults.\n\nClarity Checklist\n- Scope & NON_GOALS; user-visible behavior\n- Interfaces (API/CLI/GUI), inputs/outputs, error handling\n- Data models/storage & migrations\n- Performance/SLOs; hot paths\n- Security/Privacy/Compliance (no secrets, least privilege)\n- Observability (logs/metrics/traces, alerts)\n- Rollout strategy (feature flags, staged rollout)\n- Dependencies & compatibility\n- Test expectations (coverage, runtime, flakiness)\n- Delivery constraints (LoC/file, style, binary size)\n- Documentation/change log expectations\n- Core functionality guard (public APIs, invariants)\n\n- Phase 2 \u2014 Summarize Risks & Confirm Risk Mode:\n  \u2022 Summarize proposed changes and breakages.\n  \u2022 Call out which recommendations may negatively affect stability.\n  \u2022 Ask the user to confirm risk appetite:\n    - \u201cYOLO\u201d = accept risky changes, or\n    - \u201cSAFE: <exclusions>\u201d = exclude listed risky changes.\n\n- Phase 3 \u2014 Final Output:\n  \u2022 Produce a single refactorization meta-prompt (see Output Format).\n\nOutput Format\nEmit the following, in order:\n\n1) QUESTIONS_ROUND_1 (Q1\u2013Q5)\n2) QUESTIONS_ROUND_2 (Q6\u2013Q10)\n3) QUESTIONS_ROUND_3 (Q11\u2013Q15)\n4) SUMMARY_OF_CHANGES_AND_BREAKAGES\n   - High-level proposed changes\n   - Potential breakages/impacts\n   - Risky areas needing explicit approval\n5) RISK_CONFIRMATION_PROMPT\n   - \u201cYOLO\u201d or \u201cSAFE: <exclusions>\u201d\n6) FINAL_META_PROMPT\n\nBEGIN REFACTORIZATION META-PROMPT\n# ROLE & OBJECTIVE\nYou are the Implementer LLM. Analyze the entire codebase to identify optimizations, security flaws, and improvements. Stay environment-agnostic; propose actionable, minimal-diff refactors to improve speed, security, test coverage, and code health (including dead code removal, file splits, folder reorganization, and documentation updates).\n\n# INPUTS\n- Focus Areas: {{focus}}\n- RISK_MODE: <YOLO | SAFE with exclusions>\n\n# ASSUMPTIONS\n- Defaults taken due to unknowns.\n- Inferred budgets/policies.\n\n# NON_GOALS\n- Explicit out-of-scope areas.\n\n# CONSTRAINTS\n- Backward compatibility unless approved.\n- No secret logging; no credential changes.\n- No dependency upgrades/downgrades unless requested.\n- File/function size limits (\u2264400 LOC/file, \u226475 LOC/function).\n- Reuse-first; prefer extraction to large new modules.\n- Single-purpose modules with capability-based naming.\n\n# DESIGN_OVERVIEW\n- 4\u20137 bullets: key components, data flow, major constraints.\n\n# IMPLEMENTATION_PLAN\n- Workspace discovery of functions/classes/utilities with file paths/signatures.\n- Reuse analysis: existing logic first.\n- Size impact assessment (extract if > limits).\n- Incremental refactoring steps.\n- Ordered plan with flags, migrations, shims.\n- Dependency impact map (callers/callees).\n- Core change guard: halt & escalate if APIs/invariants shift.\n\n# DIFFS (PLANNED)\n- Per file/module: summary of changes, risk rating.\n\n# TEST_PLAN\n- Acceptance criteria mapped to tests.\n- Unit/integration/e2e; security and negative cases.\n- Coverage target and max runtime.\n- Preconditions: existing tests pass unchanged.\n\n# VALIDATION\n- Exact commands to lint, typecheck, test.\n- Smoke checks for critical flows.\n\n# SECURITY_PRIVACY_REVIEW\n- Threat model, secrets handling, authn/z, PII/retention.\n\n# PERFORMANCE_IMPACT\n- Budgets, measurement plan, rollback if P95 worsens.\n\n# OBSERVABILITY_PLAN\n- Logging strategy, metrics, error handling, alerts.\n\n# ROLLBACK_PLAN\n- Feature flags, idempotent rollback, data compatibility.\n\n# REGRESSION_PREVENTION\n- Tests, CI gates, ownership of follow-ups.\n\n# DIFFSTAT\n- Estimated files/LoC changes, risk rating, variances vs constraints.\n\n# IMPLEMENTER_OUTPUT_PROTOCOL\n- WORKSPACE_SEARCH_RESULTS\n- EXTRACTION_PLAN\n- SUMMARY (plain English)\n- DELIVERABLES (files, tests, docs)\n- IMPROVEMENT SUGGESTIONS\n- CHANGE LOG / RISKS\n- TEST RUN INSTRUCTIONS\n- LEGACY TESTS PASS PROOF\n- REFACTOR_SUMMARY (moved symbols, affected files)\n- DEPENDENCY IMPACT REPORT\n- STRUCTURE PLAN EVIDENCE\n\n# DOCUMENTATION UPDATES\n- README/CHANGELOG/API docs aligned to new state.\nEND REFACTORIZATION META-PROMPT\n\nQuality & Self-Check\n- Exactly 15 questions asked in 3 rounds.\n- Risks summarized before confirmation.\n- YOLO-first discovery, SAFE by default for recommendations.\n- Includes DESIGN_OVERVIEW, REGRESSION_PREVENTION, DIFFSTAT.\n- Priorities: security > performance > reliability > maintainability.\n- All constraints enforced explicitly.",
  "description": "This template generates refactoring meta-prompts to help you improve code quality systematically without breaking functionality. It identifies code smells, proposes refactoring strategies, and ensures test coverage throughout the process. Output: refactoring plan with before/after examples, risk assessment, and validation steps. Use when code needs restructuring, reducing technical debt, or improving maintainability.",
  "trigger": ":refactoriz",
  "variables": [
    {
      "name": "focus",
      "label": "Optional Focus Areas (e.g., security, compliance, testing, performance, languages to update, modules to restructure, documentation, test coverage, etc.)",
      "multiline": true
    }
  ]
}